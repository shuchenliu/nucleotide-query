import hashlib
import uuid
from urllib.parse import urlencode

import re2 as re
from django.core.cache import cache
from django.db import transaction
from django.utils.decorators import method_decorator
from django.views.decorators.cache import cache_page
from rest_framework import status
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework.views import APIView

from api.genome.reference import GenomeReference
from api.models import SearchTerm, Match, Search
from api.serializers import SearchTermSerializer, MatchSerializer
from api.views.query.pagination import QueryPagination


# todo:
# 2. celery for delayed db writes

class QueryView(APIView):
    parser_classes = [JSONParser]

    sequence_id = None
    sequence = None

    def initial(self, request, *args, **kwargs):
        self.sequence, self.sequence_id = GenomeReference.get()

    def get(self, request):
        # use serializer to conduct validation
        serializer = SearchTermSerializer(data=request.query_params)
        # handle invalid requests
        if not serializer.is_valid():
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)


        # cache query
        cache_key = self.get_cache_key(request)
        cached = cache.get(cache_key)

        # use cached results if present
        if cached:
            search_term_id = cached["search_term_id"]
            response = cached["response"]

            # save Search
            self.save_search(request, search_term_id)

            # use cached response
            return Response(response)


        # Try match with db
        pattern = serializer.validated_data['pattern']

        try:
            search_term = SearchTerm.objects.get(pattern=pattern)
        except SearchTerm.DoesNotExist:
            search_term = self.process_search_term(pattern)


        self.save_search(request, search_term.id)

        paginated_response = self.prepare_response(search_term)

        # cache results
        cache.set(cache_key, {
            "search_term_id": search_term.id,
            "response": paginated_response.data,
        })

        return paginated_response


    def save_search(self, request, search_term_id):
        """
        Save search actions. maybe we can get more fine-grained control using session
        but for this project excluding queries using page param should do it

        :param request: DRF Request object
        :param search_term_id: id of the search term
        """

        if 'page' not in request.query_params:
            Search.objects.create(search_term_id=search_term_id)

    def process_search_term(self, pattern):
        with transaction.atomic():
            # create SearchTerm record
            search_term = SearchTerm.objects.create(pattern=pattern, sequence_id=self.sequence_id)

            # bulk saving to db in batch:
            for batch in self.get_match_stream(pattern):
                # calculate ids
                match_ids = [
                    self.compute_match_id(start, end)
                    for start, end in batch
                ]

                # bulk create
                match_records = [
                    Match(id=mid, start=start, end=end, sequence_id=self.sequence_id)
                    for mid, (start, end) in zip(match_ids, batch)
                ]

                Match.objects.bulk_create(match_records, ignore_conflicts=True, batch_size=500)

                # add many-to-many relationships to search_term
                search_term.matches.add(*match_ids)
        return search_term

    def prepare_response(self, search_term: SearchTerm):
        matches = search_term.matches.all()
        if not matches.exists():
            return Response({
                "details": "No matches found",
            }, status=status.HTTP_404_NOT_FOUND)

        pagination = QueryPagination()
        page = pagination.paginate_queryset(matches, self.request, view=self)

        serializer = MatchSerializer(page, many=True)
        # return Response(serializer.data, status=status.HTTP_200_OK)

        return pagination.get_paginated_response(serializer.data)


    def get_cache_key(self, request, key_prefix='query'):
        param_string = urlencode(sorted(request.GET.items()))
        hash_key = hashlib.md5(param_string.encode()).hexdigest()
        key = f"{key_prefix}:{hash_key}"

        return key

    def compute_match_id(self, start: int, end: int) -> uuid.UUID:
        # predefined UUID, generated by uuid.uuid4()
        name_space = uuid.UUID('7c8d57cf-ff50-4a5b-a5fe-8710de4a2617')
        payload = f"{self.sequence_id}:{start}:{end}"
        return uuid.uuid5(name_space, payload)


    def get_match_stream(self, term, chunk_size=1000) :
        regex_pattern = re.compile(term)
        batch = []

        for matched in regex_pattern.finditer(self.sequence):
            batch.append((matched.start(), matched.end()))
            if len(batch)  == chunk_size:
                yield batch
                batch = []

        if batch:
            yield batch
