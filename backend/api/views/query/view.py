import uuid

import re2 as re
from django.db import transaction
from rest_framework import status
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework.views import APIView

from api.genome.reference import GenomeReference
from api.models import SearchTerm, Match, Search
from api.serializers import SearchTermSerializer, MatchSerializer
from api.views.query.pagination import QueryPagination
from api.views.query.query_cache_management import QueryViewCache
from api.views.search.search_cache_management import invalidate_recent_search_cache


# todo:
# 2. celery for delayed db writes

class QueryView(APIView):
    parser_classes = [JSONParser]

    sequence_id = None
    sequence = None

    def initial(self, request, *args, **kwargs):
        self.sequence, self.sequence_id = GenomeReference.get()

    def get(self, request):
        # use serializer to conduct validation
        serializer = SearchTermSerializer(data=request.query_params)
        # handle invalid requests
        if not serializer.is_valid():
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

        query_cache = QueryViewCache(request)

        # check if cached response exists
        if query_cache.check_cache_for_response():
            # save Search and use cached response
            self.save_search(request, query_cache.search_term_id)
            return query_cache.response

        # try pattern match with db
        pattern = serializer.validated_data['pattern']

        try:
            search_term = SearchTerm.objects.get(pattern=pattern)
        except SearchTerm.DoesNotExist:
            search_term = self.process_search_term(pattern)


        self.save_search(request, search_term.id)

        response = self.prepare_response(search_term)

        # save response to cache
        query_cache.cache_response(response, search_term.id)

        return response


    def save_search(self, request, search_term_id):
        """
        Save search actions. maybe we can get more fine-grained control using session
        but for this project excluding queries using page param should do it

        :param request: DRF Request object
        :param search_term_id: id of the search term
        """

        if 'page' not in request.query_params:
            Search.objects.create(search_term_id=search_term_id)
            # saving search affect the results of recent searches
            invalidate_recent_search_cache()

    def process_search_term(self, pattern):
        with transaction.atomic():
            # create SearchTerm record
            search_term = SearchTerm.objects.create(pattern=pattern, sequence_id=self.sequence_id)

            # bulk saving to db in batch:
            for batch in self.get_match_stream(pattern):
                # calculate ids
                match_ids = [
                    self.compute_match_id(start, end)
                    for start, end in batch
                ]

                # bulk create
                match_records = [
                    Match(id=mid, start=start, end=end, sequence_id=self.sequence_id)
                    for mid, (start, end) in zip(match_ids, batch)
                ]

                Match.objects.bulk_create(match_records, ignore_conflicts=True, batch_size=500)

                # add many-to-many relationships to search_term
                search_term.matches.add(*match_ids)
        return search_term

    def prepare_response(self, search_term: SearchTerm):
        matches = search_term.matches.all()
        if not matches.exists():
            return Response(data={
                "details": "No matches found",
            }, status=status.HTTP_404_NOT_FOUND)

        pagination = QueryPagination()
        page = pagination.paginate_queryset(matches, self.request, view=self)

        serializer = MatchSerializer(page, many=True)
        # return Response(serializer.data, status=status.HTTP_200_OK)

        return pagination.get_paginated_response(serializer.data)


    def compute_match_id(self, start: int, end: int) -> uuid.UUID:
        # predefined UUID, generated by uuid.uuid4()
        name_space = uuid.UUID('7c8d57cf-ff50-4a5b-a5fe-8710de4a2617')
        payload = f"{self.sequence_id}:{start}:{end}"
        return uuid.uuid5(name_space, payload)


    def get_match_stream(self, term, chunk_size=1000) :
        regex_pattern = re.compile(term)
        batch = []

        for matched in regex_pattern.finditer(self.sequence):
            batch.append((matched.start(), matched.end()))
            if len(batch)  == chunk_size:
                yield batch
                batch = []

        if batch:
            yield batch
