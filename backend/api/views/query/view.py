import uuid

import re2 as re
from django.db import transaction
from rest_framework import status
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework.views import APIView

from api.genome.reference import GenomeReference
from api.models import SearchTerm, Match, Search
from api.serializers import SearchTermSerializer, MatchSerializer
from api.views.query.pagination import QueryPagination


class QueryView(APIView):
    parser_classes = [JSONParser]

    sequence_id = None
    sequence = None

    def initial(self, request, *args, **kwargs):
        self.sequence, self.sequence_id = GenomeReference.get()

    def get(self, request):

        # use serializer to conduct validation
        serializer = SearchTermSerializer(data=request.query_params)
        # handle invalid requests
        if not serializer.is_valid():
            return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)


        # Try match with db
        # todo:
        #  1. cache layer redis
        #  2. celery for delayed db writes

        pattern = serializer.validated_data['pattern']

        try:
            search_term = SearchTerm.objects.get(pattern=pattern)
        except SearchTerm.DoesNotExist:
            search_term = self.process_search_term(pattern)

        # save initial search actions
        # maybe we can get more fine-grained control using session
        # but for this project excluding queries using page param should do it

        if 'page' not in request.query_params:
            Search.objects.create(search_term=search_term)

        return self.prepare_response(search_term)


    def process_search_term(self, pattern):
        with transaction.atomic():
            # create SearchTerm record
            search_term = SearchTerm.objects.create(pattern=pattern, sequence_id=self.sequence_id)

            # bulk saving to db in batch:
            for batch in self.get_match_stream(pattern):
                # calculate ids
                match_ids = [
                    self.compute_match_id(start, end)
                    for start, end in batch
                ]

                # bulk create
                match_records = [
                    Match(id=mid, start=start, end=end, sequence_id=self.sequence_id)
                    for mid, (start, end) in zip(match_ids, batch)
                ]

                Match.objects.bulk_create(match_records, ignore_conflicts=True, batch_size=500)

                # add many-to-many relationships to search_term
                search_term.matches.add(*match_ids)
        return search_term

    def prepare_response(self, search_term: SearchTerm):
        matches = search_term.matches.all()
        if not matches.exists():
            return Response({
                "details": "No matches found",
            }, status=status.HTTP_404_NOT_FOUND)

        pagination = QueryPagination()
        page = pagination.paginate_queryset(matches, self.request, view=self)

        serializer = MatchSerializer(page, many=True)
        # return Response(serializer.data, status=status.HTTP_200_OK)

        return pagination.get_paginated_response(serializer.data)



    def compute_match_id(self, start: int, end: int) -> uuid.UUID:
        # predefined UUID, generated by uuid.uuid4()
        name_space = uuid.UUID('7c8d57cf-ff50-4a5b-a5fe-8710de4a2617')
        payload = f"{self.sequence_id}:{start}:{end}"
        return uuid.uuid5(name_space, payload)


    def get_match_stream(self, term, chunk_size=1000) :
        regex_pattern = re.compile(term)
        batch = []

        for matched in regex_pattern.finditer(self.sequence):
            batch.append((matched.start(), matched.end()))
            if len(batch)  == chunk_size:
                yield batch
                batch = []

        if batch:
            yield batch
